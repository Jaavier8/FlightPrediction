version: "2.1"

services:
  zookeeper:
    image: docker.io/bitnami/zookeeper:3.8
    ports:
      - "2181:2181"
    #volumes:
    #  - "zookeeper_data:/bitnami"
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes

  kafka:
    image: docker.io/bitnami/kafka:3.3
    ports:
      - "9092:9092"
    #volumes:
    #  - "kafka_data:/bitnami"
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
    depends_on:
      - zookeeper

  # To create the topic flight_delay_classification_request
  # This service execute command and exit with code 0
  kafka-init:
    image: docker.io/bitnami/kafka:3.3
    depends_on:
      - kafka
    entrypoint: [ 'bin/sh', '-c' ]
    command: |
      "
      # blocks until kafka is reachable
      sleep 5

      echo -e 'Creating kafka topics'
      kafka-topics.sh --bootstrap-server kafka:9092 --create --if-not-exists --topic flight_delay_classification_request --replication-factor 1 --partitions 1

      echo -e 'Successfully created the following topics:'
      kafka-topics.sh --bootstrap-server kafka:9092 --list
      "
  webapp:
    build: "./webapp"
    ports:
      - "5000:5000"

  mongodb:
    image: "mongo"
    ports:
      - "27017:27017"
    environment:
      - MONGO_INITDB_DATABASE=agile_data_science #Access with mongodb://admin:password@127.0.0.1:27017/agile_data_science
      - MONGO_INITDB_ROOT_USERNAME=admin
      - MONGO_INITDB_ROOT_PASSWORD=password

  mongodb-init:
    image: "mongo"
    depends_on:
      - mongodb
    entrypoint: [ 'bin/sh', '-c' ]
    command: |
      "
      # blocks until mongodb is reachable
      sleep 5

      apt-get update
      apt-get install -y curl

      echo -e 'Downloading data'
      curl -Lko ./origin_dest_distances.jsonl http://s3.amazonaws.com/agile_data_science/origin_dest_distances.jsonl

      echo -e 'Importing resources'
      mongoimport --authenticationDatabase=admin --uri="mongodb://admin:password@mongodb:27017/agile_data_science" -d agile_data_science -c origin_dest_distances --file origin_dest_distances.jsonl
      mongosh -u "admin" -p "password" mongodb://admin:password@mongodb:27017/agile_data_science --authenticationDatabase "admin" --eval 'db.origin_dest_distances.createIndex({Origin: 1, Dest: 1})'
      "

  # master:
  #   image: bitnami/spark:3.1.2-debian-10-r130
  #   command: bin/spark-class org.apache.spark.deploy.master.Master -h master
  #   hostname: master
  #   environment:
  #     MASTER: spark://master:7077
  #     SPARK_CONF_DIR: /conf
  #     SPARK_PUBLIC_DNS: localhost
  #   expose:
  #     - 7001
  #     - 7002
  #     - 7003
  #     - 7004
  #     - 7005
  #     - 7077
  #     - 6066
  #   ports:
  #     - 4040:4040
  #     - 6066:6066
  #     - 7077:7077
  #     - 8080:8080
  #   volumes:
  #     - ./docker-spark/conf/master:/conf
  #     - ./docker-spark/data:/tmp/data

  # worker:
  #   image: bitnami/spark:3.1.2-debian-10-r130
  #   command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://master:7077
  #   hostname: worker
  #   environment:
  #     SPARK_CONF_DIR: /conf
  #     SPARK_WORKER_CORES: 2
  #     SPARK_WORKER_MEMORY: 1g
  #     SPARK_WORKER_PORT: 8881
  #     SPARK_WORKER_WEBUI_PORT: 8081
  #     SPARK_PUBLIC_DNS: localhost
  #   links:
  #     - master
  #   expose:
  #     - 7012
  #     - 7013
  #     - 7014
  #     - 7015
  #     - 8881
  #   ports:
  #     - 8081:8081
  #   volumes:
  #     - ./docker-spark/conf/worker:/conf
  #     - ./docker-spark/data:/tmp/data

  spark:
    image: bitnami/spark:3.1.2-debian-10-r130
    user: root
    environment:
      - SPARK_MODE=master
    ports:
      - '8080:8080'
      - '7077:7077'
    volumes:
      #- /home/javier/Escritorio/BDFI/ProyectoFinal/practica_big_data_2019/flight_prediction/target/scala-2.12:/opt/spark-apps
      - shared-volume:/opt/spark-apps
      - ./predictor/jars_dir:/opt/bitnami/spark/ivy:z

  spark-worker:
    image: bitnami/spark:3.1.2-debian-10-r130
    deploy:
      replicas: 3
    user: root
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=2
    #ports:
    #  - '8081:8081'

    volumes:
      #- /home/javier/Escritorio/BDFI/ProyectoFinal/practica_big_data_2019/flight_prediction/target/scala-2.12:/opt/spark-apps
      - shared-volume:/opt/spark-apps
      - ./predictor/jars_dir:/opt/bitnami/spark/ivy:z

  spark-init:
    image: bitnami/spark:3.1.2-debian-10-r130
    user: root
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
    volumes:
      #- /home/javier/Escritorio/BDFI/ProyectoFinal/practica_big_data_2019/flight_prediction/target/scala-2.12:/opt/spark-apps
      - shared-volume:/opt/spark-apps
    depends_on:
      - spark
    entrypoint: [ 'sh', '-c' ]
    command: |
      "
      #Sleep until sbt compile and package
      sleep 60

      echo -e 'Starting predictor'
      spark-submit --conf spark.jars.ivy=/opt/bitnami/spark/ivy --master spark://spark:7077 --deploy-mode cluster --supervise --class es.upm.dit.ging.predictor.MakePrediction --name FlightPrediction --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 /opt/spark-apps/practicaBD/flight_prediction/target/scala-2.12/flight_prediction_2.12-0.1.jar
      "

  predictor:
    build: "./predictor"
    volumes:
      - shared-volume:/opt/spark-apps

volumes:
  shared-volume:
#volumes:
#  zookeeper_data:
#    driver: local
#  kafka_data:
#    driver: local
