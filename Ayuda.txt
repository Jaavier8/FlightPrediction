*******spark-submit********

-https://sparkbyexamples.com/spark/spark-submit-command/
-~/Escritorio/BDFI/ProyectoFinal/spark/spark$ ./bin/spark-submit --help

Funcionamiento correcto: +1
-javier@javier:~/Escritorio/BDFI/ProyectoFinal/spark/spark/bin$ ./spark-submit --master local --deploy-mode client --class es.upm.dit.ging.predictor.MakePrediction --name FlightPrediction --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 ../../../practica_big_data_2019/flight_prediction/target/scala-2.12/flight_prediction_2.12-0.1.jar

Antes de la ejecución, hay que generar el fichero jar que se pasa como argumento. El fichero jar se obtiene con sbt. Para ello, nos colocamos en la carpeta raíz del proyecto (~/Escritorio/BDFI/ProyectoFinal/practica_big_data_2019) y ejecutamos: 'sbt compile' y 'sbt package'. El fichero se guarda en target/scala-2.12/flight_prediction_2.12-0.1.jar






*******dockerizar********
-Kafka_zookeeper: https://hub.docker.com/r/bitnami/kafka
-create kafka topic: https://stackoverflow.com/questions/64865361/docker-compose-create-kafka-topics

-mongodb: https://faun.pub/managing-mongodb-on-docker-with-docker-compose-26bf8a0bbae3
-Para llenar la base de datos hay que ejecutar el script import_distances.sh, lo haremos desde otro contenedor init(como en el caso de kafka)
-Probando desde el host, se ejecuta el siguiente comando: 
    -mongoimport --authenticationDatabase=admin --uri="mongodb://admin:password@127.0.0.1:27017/agile_data_science" -d agile_data_science -c origin_dest_distances --file ../practica_big_data_2019/data/origin_dest_distances.jsonl
    -mongo -u "admin" -p "password" mongodb://admin:password@127.0.0.1:27017/agile_data_science --authenticationDatabase "admin" --eval 'db.origin_dest_distances.createIndex({Origin: 1, Dest: 1})'
-Para probar, una vez levantado mongo:
    -docker exec -it "container" mongosh
    -use admin
    -db.auth("admin","password")
    -use agile_data_science
    -show tables
    -db.origin_dest_distances.find()

-mongodbinit: necesario para ejecutar los comandos de las líneas 18 y 19 (descargar datos que se importan en mongo y crear índice)

-webapp: cambios en predict_flask.py
    - línea 15 para indicar dirección de mongodb (hay que pasar así la dirección ya que lleva la autenticación implicita)
    - línea 28 para indicar dirección de kafka

-spark:
-javier@javier:~/Escritorio/BDFI/ProyectoFinal/Mejora_Docker$ ./../spark/spark/bin/spark-submit --master spark://127.0.0.1:7077 --deploy-mode cluster --class es.upm.dit.ging.predictor.MakePrediction --name FlightPrediction --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 ../practica_big_data_2019/flight_prediction/target/scala-2.12/flight_prediction_2.12-0.1.jar
    -Primer error encontrado: Si ejecuto la instrucción anterior desde el host, los workers de spark buscan el jar en /home/javier/Escritorio/BDFI/ProyectoFinal/Mejora_Docker/../practica_big_data_2019/flight_prediction/target/scala-2.12/flight_prediction_2.12-0.1.jar, que es donde se encuentra en el host, y lógicamente, dentro de la máquina de spark, eso no existe.
-Por este motivo, la instrucción se tiene que ejecutar desde algún sitio que comparta almacenamiento con spark. SOLUCIÓN: un contenedor que haga la llamada y que tenga volumen compartido con spark
    -Segundo error encontrado: el driver que se ejecuta falla debido a que no encunetra una clase
        -Motivo y solución: el contenedor no tenía spark 3.1.2, por lo que hay que cambiar a uno con spark 3.1.2
    -Tercer error encontrado: el driver falla porque no exixte /opt/bitnami/spark/.ivy2...
        -Motivo y solución: al ejecutar spark-submit con la instrucción --packages utiliza ese directorio. Como el directorio es read only, falla. Para ello hay que tener un volumen compartido con el host e indicarle a spark-submit que --conf es ese directorio
        -https://exchangetuts.com/i-cannot-use-package-option-on-bitnamispark-docker-container-1640444943522761
        -https://github.com/bitnami/bitnami-docker-spark/issues/7
-Comando a ejecutar: spark-submit --conf spark.jars.ivy=/opt/bitnami/spark/ivy --master spark://spark:7077 --deploy-mode cluster --class es.upm.dit.ging.predictor.MakePrediction --name FlightPrediction --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 /opt/spark-apps/practica_big_data_2019/flight_prediction/target/scala-2.12/flight_prediction_2.12-0.1.jar
    -Cuarto error encontrado: La página web envía el vuelo a kafka, spark streaming lo coge y hace la predicción (se puede ver entrando al contenedor de spark, dentro de work y del driver en stdout o en la interfaz web en el navegador) pero no se guarda en mongodb y por lo tanto no llega a la web.
    -Motivo y solución: en la línea 142 de MakePrediction.scala se pasa la dirección de mongodb, pero no autorización (descubierto al entrar en el contenedor de spark, abrir una consola scala y ejecutar los comandos siguientes). Para autorizar hay que añadir al final ?authSource=admin
    https://stackoverflow.com/questions/39576857/mongo-spark-connector-and-mongo-3-2-root-user-cannot-read-database
        -Encontrar fallo:
            -Abro contenedor spark
            -Ejecuto: spark-shell --conf spark.jars.ivy=/opt/bitnami/spark/ivy --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2
            -Importo lo necesario para MongoSpark, WriteConfig y import org.bson.Document
            -val writeConfig = WriteConfig(Map("uri" -> "mongodb://admin:password@mongodb:27017/agile_data_science.flight_delay_classification_response"))
            -val sparkDocuments = sc.parallelize((1 to 10).map(i => Document.parse(s"{spark: $i}")))
            -MongoSpark.save(sparkDocuments, writeConfig)
            -Y obtenemos error de autenticación





*******airflow********
-Cuando se descarga sqlite3 hay que ejecutar: 'airflow db init' para que cree la tabla necesaria
-Para que cree el dag sólo hay que copiar el setup.py en ~/airflow/dags
-Para lanzar el dag: airflow dags trigger agile_data_science_batch_prediction_model_training
-Para poder ejecutar spark-submit desde cualquier sitio (porque se utiliza en setup.py), hay que meterlo en el PATH => export PATH=$PATH:/home/javier/Escritorio/BDFI/ProyectoFinal/spark/spark/bin/
-IMPORTANTE: hacer el EXPORT PROJECT_HOME tatnto en la consola que se levanta el webserver como en la consola que se levanta el scheduler, si no no funciona
-En la línea 63 se le pasa el atributo dag al BashOperator. El DAG tiene los argumentos por defecto. Como se puede ver en la línea 12, si falla se reintenta hasta 3 veces, y lo hace cada 5 minutos.